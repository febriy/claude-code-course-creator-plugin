# DEFAULT WRITING STYLE GUIDE
## Educational Content - Professional-Conversational Style

**Generated:** 2025-10-27
**Based on:** 3 good examples (technical education textbooks) + 3 bad examples (academic papers, newsletters)
**For use by:** Writers and editors creating educational course content

---

## OVERVIEW

**Style Summary:**
This is a professional-conversational educational style that teaches complex concepts with clarity, credibility, and engagement. It balances technical accuracy with approachabilityâ€”like an expert professor who cares about student understanding. The writing is research-backed, uses concrete examples, and maintains academic credibility while avoiding both overly casual marketing tone and impenetrable academic jargon.

**Target Audience:**
Intelligent adult learners (professionals, students) approaching technical topics without domain expertise. They can handle complexity when properly explained, value evidence-based teaching, and want practical understanding alongside theoretical knowledge.

**Primary Goal:**
Make complex technical concepts understandable, memorable, and credible through clear explanation, concrete examples, research grounding, and strategic engagementâ€”all while maintaining professional educational standards.

---

## TONE & VOICE

### Do This âœ…

**Tone Characteristics:**
- **Professional-Conversational**: Expert authority balanced with warmth and accessibility
- **Evidence-Based**: Claims backed by research, not just opinion
- **Teaching-First**: Every element serves learning, not marketing or showing off
- **Respectfully Direct**: Clear and concise without being casual or condescending
- **Credibly Engaging**: Uses rhetorical techniques strategically, not gimmicky

**Voice Guidelines:**
- Use "you" to directly address learner
- Occasional "we" for shared exploration ("we'll discover...")
- Use "I" sparingly for personal guidance ("I encourage you...")
- Avoid contractions in most cases (exception: occasional natural flow)
- Include thoughtful prompts to engage thinking
- Express genuine intellectual curiosity
- Acknowledge both complexity and possibility

**Example (GOOD - from Good #1):**
> "Supervised machine learning is the bread and butter of applied artificial intelligence (also known as narrow AI). It's useful for taking what you know as input and quickly transforming it into what you want to know."

**Why it works:**
- Accessible metaphor ("bread and butter")
- Parenthetical clarification
- Natural contraction for flow
- Explains practical value
- Professional but not stuffy

**Example (GOOD - from Good #2):**
> "Simply prompting an LLM can be a useful approach to answer many factoid questions. But the fact that knowledge is stored in the feedforward weights of the LLM leads to a number of problems with prompting as a method for correctly answering factual questions."

**Why it works:**
- States benefit first, then limitation
- Technical but clear
- Logical progression
- No unnecessary jargon
- Educational, not performative

### Don't Do This âŒ

**Avoid:**
- **Overly casual**: "just really fun", "gonna", "wanna", "super cool"
- **Marketing speak**: "amazing", excessive CTAs, promotional language
- **Academic pomposity**: "one might posit", "it behooves us", unnecessarily complex phrasing
- **Dense jargon dumps**: Technical terms without explanation or context
- **Condescension**: "don't worry", "it's easy", "simply"
- **Fake enthusiasm**: Multiple exclamation marks!!!, CAPS, emoji spam

**Example (BAD - from Bad #1 - Newsletter):**
> "This is a great app to build if you're new to AI coding because it's just really fun to create professional headshots for your family and friends. I've made this tutorial 100% free..."

**Why it's bad:**
- Too casual ("just really fun")
- Marketing language ("100% free")
- Personal promotion over teaching
- Lacks substance
- Newsletter/blog tone, not educational

**Example (BAD - from Bad #2 - Academic Paper):**
> "Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn)."

**Why it's bad:**
- Assumes expert knowledge
- No explanation of terms
- Mathematical notation without context
- No examples or grounding
- Impenetrable to learners

**Example (BAD - from Bad #3 - Tutorial):**
> "Dear subscribers, Today, I want to show you how to build an app... P.S. I wake up at 6 am to make these tutorials so subscribe to my YouTube if you enjoy them. ðŸ™"

**Why it's bad:**
- Newsletter greeting style
- Self-promotional
- Emoji usage
- Subscription pitch
- Too personal/casual for educational content

---

## SENTENCE STRUCTURE

### Do This âœ…

**Guidelines:**
- Target sentence length: 18-25 words average (range: 8-40 words)
- Active voice: 85%+ of sentences
- Sentence variety: Mix short statements (8-15 words) with longer explanations (25-35 words)
- One core idea per sentence, even if complex
- Use subordinate clauses for context, not confusion

**Patterns that work:**

**Pattern 1: Short statement â†’ Longer explanation with example**
```
Example: "Large language models hallucinate. A hallucination is a response that is not faithful to the facts of the world. That is, when asked questions, large language models sometimes make up answers that sound reasonable."
```

**Pattern 2: Problem â†’ Evidence â†’ Solution**
```
Example: "The first and main problem is that LLMs often give the wrong answer to factual questions! Large language models hallucinate. [...] One solution to all these problems with simple prompting for answering factual questions is to give a language model external sources of knowledge."
```

**Pattern 3: Concept â†’ Concrete Example â†’ Implication**
```
Example: "Estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used. A simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day."
```

**Pattern 4: Engaging question â†’ Thoughtful answer**
```
Example: "What can language models learn from word prediction? Consider the examples below. What kinds of knowledge do you think the model might pick up?"
```

### Don't Do This âŒ

**Avoid:**
- Run-on sentences (>45 words without clear structure)
- Passive voice overuse (keep <15%)
- Choppy repetitive structure (all short or all long)
- Dense academic constructions that obscure meaning
- Casual fragments that seem lazy

**Example (BAD - Dense Academic):**
> "The encoder is composed of a stack of N = 6 identical layers each having two sub-layers where the first is a multi-head self-attention mechanism and the second is a simple position-wise fully connected feed-forward network with residual connections employed around each sub-layer followed by layer normalization."

**Fixed version:**
> "The encoder has a stack of 6 identical layers. Each layer contains two sub-layers: a multi-head self-attention mechanism and a feed-forward network. We add residual connections around each sub-layer, followed by layer normalization."

---

## WORD CHOICE

### Do This âœ…

**Vocabulary Level:** Educated adult (university-level reading) but favoring clarity over complexity

**Preferred Words:**

| Concept | Use This | Not This |
|---------|----------|----------|
| Using | use | utilize, leverage |
| Helping | help, enable | facilitate |
| Change | transform, change | modify, alter |
| Method | method, approach | methodology |
| Show | show, demonstrate | illustrate |
| Fast | quickly, fast | expeditiously |
| Build | build, create | construct, fabricate |
| Real | actual, real | authentic, genuine |

**Technical Terms Handling:**
- **Define on first use** with inline explanation or parenthetical
- **Use freely after definition** - don't keep re-explaining
- **Provide context** before diving into technical detail
- **Build complexity gradually** - simple to complex
- **Examples ground abstractions**

**Examples of Good Technical Term Introduction:**

From Good #2:
> "This method is called retrieval-augmented generation or RAG"

From Good #1:
> "Supervised machine learning is the bread and butter of applied artificial intelligence (also known as narrow AI)"

**Common Effective Phrases:**
- "For example," - Introduces concrete examples
- "That is," - Clarifies or rephrases
- "Consider..." - Invites thinking
- "Research shows..." - Introduces evidence
- "This means..." - Shows practical implication

### Don't Do This âŒ

**Words to Avoid:**

**Corporate Jargon:**
- "leverage" â†’ use
- "utilize" â†’ use
- "facilitate" â†’ help, enable
- "synergize" â†’ work together
- "paradigm" â†’ model, approach
- "methodology" â†’ method

**Vague Intensifiers:**
- "very", "really", "quite", "somewhat"
- Use precise language instead

**Casual/Informal:**
- "gonna", "wanna", "kinda"
- "just" (when minimizing)
- "super", "awesome", "amazing"

**Unnecessarily Complex:**
- "utilize" â†’ use
- "elucidate" â†’ explain
- "endeavor" â†’ try
- "ascertain" â†’ find out

---

## FORMATTING & STRUCTURE

### Do This âœ…

**Paragraph Guidelines:**
- Length: 2-4 sentences (40-120 words)
- One main idea per paragraph
- Occasional single-sentence paragraphs for emphasis or transition
- White space between paragraphs (single or double line breaks)
- Topic sentence establishes focus

**Header Usage:**
- Headers every 4-6 paragraphs
- Descriptive, functional headers (not cute or clever)
- Hierarchical: main concepts (##), sub-concepts (###)
- Example good headers:
  - "Supervised machine learning"
  - "What is rigorous thinking?"
  - "RAG: retrieval-augmented generation"

**Lists:**
- Use for actual lists of items
- Prefer prose for explanations and examples
- Numbered lists for steps/sequences
- Bullet lists for unordered items
- Keep list items parallel in structure

**Emphasis:**
- **Bold** for: Key technical terms on first introduction
- *Italic* for: Emphasis within sentences (rare)
- Avoid: ALL CAPS (except acronyms), excessive formatting

**Citations:**
- Numbered in-text citations: [1], [2]
- References section at bottom
- Format: Author/Organization, "Title", Year
- NO URLs in learner-facing content

**Example (GOOD Structure):**
```markdown
## Information Retrieval

People need to know things. So pretty much as soon as there were computers, we were asking them questions. By 1961 there was a system to answer questions about American baseball statistics. [1]

It follows naturally that an important function of large language models is to fill human information needs by answering questions. Consider some simple factoid questions that can be answered with facts expressed in short texts.

---
**References**

[1] Green et al., "Baseball Question Answering System", 1961
```

### Don't Do This âŒ

**Avoid:**
- Walls of text (8+ sentences without breaks)
- Too many headers (breaking natural flow)
- Bullet points for everything
- No white space
- Newsletter formatting (subscription boxes, CTAs)
- Excessive bold/italic/formatting

---

## CONTENT ORGANIZATION

### Do This âœ…

**Preferred Structure Pattern:**
```
HOOK â†’ CONCEPT â†’ DEFINITION â†’ EXAMPLE â†’ EVIDENCE â†’ APPLICATION â†’ ENGAGEMENT

Example outline:
1. Hook with relevance or problem
2. Introduce concept clearly
3. Define in plain language
4. Provide concrete example(s)
5. Support with research/evidence
6. Show practical application
7. Engage reader (question, reflection, forward reference)
```

**Opening Patterns:**

**Pattern A: Historical/Contextual Hook**
> "People need to know things. So pretty much as soon as there were computers we were asking them questions."

**Pattern B: Problem Statement**
> "The first and main problem is that LLMs often give the wrong answer to factual questions!"

**Pattern C: Direct Definition with Context**
> "Supervised learning is a method for transforming one dataset into another."

**Development Patterns:**
- Move from abstract to concrete quickly
- Use "For example," to signal transitions
- Build complexity gradually
- Connect new concepts to previous explanations
- Layer evidence throughout

**Evidence Integration:**
- Research findings stated as facts: "Research suggests that..."
- Citations follow claims: "...vocabulary growth. [1]"
- Statistics with context: "from 69% to 88% of the time"
- Examples ground theory

**Engagement Techniques:**
- **Thought prompts**: "Think about this for each example before you read ahead."
- **Forward references**: "you'll learn more about in a moment"
- **Rhetorical questions**: "How do children achieve this rate?"
- **Invitations**: "Consider some simple factoid questions"
- **Genuine excitement**: "Such an ability would be profound."

### Don't Do This âŒ

**Avoid:**
- Starting with dense theory before any grounding
- Examples without context or setup
- Abrupt topic changes
- Over-explaining transitions
- Marketing hooks or promotional openings
- Newsletter greetings ("Dear subscribers")

---

## EXAMPLES & EVIDENCE

### Do This âœ…

**When to Use Examples:**
- Immediately after introducing abstract concepts
- To ground technical ideas
- To show practical application
- Before diving into complexity

**How to Introduce Examples:**
- "For example," (most common)
- "Consider..." (invites thinking)
- Integrate directly without announcement
- Build from simple to complex

**Example Characteristics:**
- **Specific and concrete**: "Monday Stock Prices dataset over 10 years"
- **Realistic**: Could actually exist or occur
- **Complete**: Full scenario, not fragments
- **Relatable**: Connects to learner experience when possible
- **Progressive**: Simple first, complex later

**Evidence Patterns:**

**Pattern 1: Research statement + citation**
> "Research suggests that the bulk of this knowledge acquisition happens as a by-product of reading."

**Pattern 2: Statistical claim + citation**
> "Dahl et al. (2024) found that when asked questions about the legal domain, large language models hallucinated from 69% to 88% of the time!"

**Pattern 3: Factual statement + inline citation**
> "By 1961 there was a system to answer questions about American baseball statistics. [1]"

**Examples from Good Samples:**

From Good #1:
> "For example, if you had a dataset called Monday Stock Prices that recorded the price of every stock on every Monday for the past 10 years, and a second dataset called Tuesday Stock Prices..."

From Good #2:
> "For example, estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used."

### Don't Do This âŒ

**Avoid:**
- Vague examples without specifics
- Examples more complex than the concept
- Too many examples without explanation between them
- Made-up examples that strain credibility
- Examples without clear connection to concept

---

## CITATIONS & RESEARCH GROUNDING

### Do This âœ…

**Citation Style:**
- **In-text**: Numbered brackets [1], [2] after claims
- **OR**: Author/org inline: "Dahl et al. (2024) found that..."
- **References**: At bottom, format: Author, "Title", Year
- **NO URLs** in learner-facing content (track separately)

**What Needs Citations:**
- Statistics and data: "69% to 88% of the time [1]"
- Research findings: "Research suggests... [1]"
- Historical facts: "By 1961 there was a system... [1]"
- Expert claims and predictions
- Specific examples from studies

**What Doesn't Need Citations:**
- Common knowledge
- Definitions of standard terms
- Your own explanations
- General examples without specific data

**Example (GOOD - from Good #2):**
> "Large language models hallucinate. A hallucination is a response that is not faithful to the facts of the world. That is, when asked questions, large language models sometimes make up answers that sound reasonable. For example, Dahl et al. (2024) found that when asked questions about the legal domain, large language models hallucinated from 69% to 88% of the time!"

**Why it works:**
- Defines term first (no citation needed)
- Provides specific research finding with citation
- Gives concrete data (69-88%)
- Inline citation style, clear attribution

### Don't Do This âŒ

**Avoid:**
- No citations for factual claims
- Vague attribution ("studies show")
- Made-up statistics
- Academic citation overload
- URLs in main text

---

## RHETORICAL TECHNIQUES

### Do This âœ…

**Effective Techniques:**

**1. Questions (Strategic Use):**
- Rhetorical: "How do children achieve this rate of vocabulary growth?"
- Direct: "What can language models learn from word prediction?"
- Invitation: "Think about this for each example"
- Use ~1 per major section

**2. Metaphors & Analogies:**
- Simple, universal: "bread and butter"
- Clarifying, not decorative
- Grounded in shared experience
- Examples: "clustering is something you can reliably hold onto"

**3. Invitations to Reflect:**
- "Consider the examples below"
- "Think about this before you read ahead"
- "I encourage you to stop and consider this for a moment"

**4. Genuine Enthusiasm:**
- "Such an ability would be profound"
- "I have good news!"
- Reserved for genuinely significant points

**5. Forward References:**
- "you'll learn more about in a moment"
- "which you'll discover later"
- Creates anticipation, shows structure

**6. Problem â†’ Solution Arc:**
- State problem clearly
- Build urgency/importance
- Present solution methodically
- Common in textbook writing

### Don't Do This âŒ

**Avoid:**
- Excessive rhetorical questions (feels interrogative)
- Forced analogies that confuse
- Fake enthusiasm ("Amazing!!!", "Mind-blowing!!!")
- Patronizing encouragement ("See? That wasn't hard!")
- Too many forward references (creates confusion)

---

## AUDIENCE CONSIDERATIONS

### Do This âœ…

**Assumed Reader:**
- Intelligent adult (professional or student)
- No domain expertise, but capable of learning complexity
- Values both understanding AND practical application
- Wants credible, evidence-based information
- Can handle technical content when properly explained

**Knowledge Assumptions:**
- Basic reading comprehension and logical thinking
- NO technical background in the subject
- Can learn new technical terms when defined
- Appreciates both theory and practice

**Adaptation Guidelines:**

**Technical Terms:**
- Always define on first use
- Use inline explanations or parentheticals
- Build from simple to complex
- Example: "RAG (retrieval-augmented generation)"

**Explanations:**
- Thorough but efficient
- Don't skip logical steps
- Don't over-explain obvious things
- Respect intelligence while providing clarity

**Respect for Reader:**
- Trust them to follow logical arguments
- Invite reflection, don't spoon-feed
- Acknowledge complexity honestly
- Encourage active thinking

**Example (GOOD):**
> "What made the modern NLP revolution possible is that large language models can learn all this knowledge of language, context, and the world simply by being taught to predict the next word, again and again, based on context, in a (very) large corpus of text."

**Why it works:**
- Explains complex idea clearly
- No condescension
- Acknowledges significance
- Builds from known to unknown

### Don't Do This âŒ

**Avoid:**
- Assuming expert knowledge without building up
- Explaining trivial concepts
- Condescending language
- Talking down ("Let me make this simple for you")
- Marketing language ("You'll love this!")

---

## QUICK REFERENCE CHECKLIST

### Before Publishing, Check:

**Tone:**
- [ ] Professional but conversational (not stuffy, not too casual)
- [ ] Uses "you" to engage reader
- [ ] Evidence-based claims with citations
- [ ] Teaching-first (not marketing, not showing off)
- [ ] Genuine enthusiasm for genuinely significant ideas

**Structure:**
- [ ] Sentence length: 18-25 words average (mix of 8-40)
- [ ] Active voice: >85%
- [ ] Paragraphs: 2-4 sentences
- [ ] White space between paragraphs
- [ ] Headers every 4-6 paragraphs

**Content:**
- [ ] Follows CONCEPT â†’ DEFINITION â†’ EXAMPLE â†’ EVIDENCE pattern
- [ ] Technical terms defined on first use
- [ ] Concrete examples with specific details
- [ ] Citations for all factual claims
- [ ] Engagement technique every major section

**Word Choice:**
- [ ] Plain language (use, help, show vs utilize, facilitate)
- [ ] No corporate jargon (leverage, synergize, paradigm)
- [ ] No casual slang (gonna, wanna, super, just)
- [ ] No vague intensifiers (very, really, quite)
- [ ] Technical terms only when necessary

**Citations:**
- [ ] All statistics and research findings cited
- [ ] Citation format consistent [1] or (Author, Year)
- [ ] References section at bottom
- [ ] No URLs in main content

**Overall:**
- [ ] Sounds like an expert teacher, not a marketer or academic paper
- [ ] Balances credibility with accessibility
- [ ] Evidence-based without being dense
- [ ] Engaging without being gimmicky

---

## STYLE COMPARISON EXAMPLES

### Example 1: Explaining a Technical Limitation

**GOOD (from Good #2):**
```
The first and main problem is that LLMs often give the wrong answer to factual questions! Large language models hallucinate. A hallucination is a response that is not faithful to the facts of the world. That is, when asked questions, large language models sometimes make up answers that sound reasonable. For example, Dahl et al. (2024) found that when asked questions about the legal domain, large language models hallucinated from 69% to 88% of the time!
```

**Why it works:**
- Clear problem statement with emphasis (!)
- Defines technical term (hallucination)
- Concrete example with specific data
- Research citation
- Accessible language
- Progressive explanation

**BAD (Academic Paper Style - Bad #2):**
```
The encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time.
```

**Why it fails:**
- Assumes expert knowledge
- Mathematical notation without context
- No explanation of terms
- No examples or grounding
- Impenetrable to learners

---

### Example 2: Engaging the Reader

**GOOD (from Good #3):**
```
What can language models learn from word prediction? Consider the examples below. What kinds of knowledge do you think the model might pick up from learning to predict what word fills the underbar? Think about this for each example before you read ahead to the next paragraph.
```

**Why it works:**
- Direct question engages thinking
- Invitation to consider
- Active participation prompt
- Respectful pacing (think before reading ahead)
- Sets up learning moment

**BAD (Newsletter Style - Bad #3):**
```
Dear subscribers, Today, I want to show you how to build an app that turns your photos into professional headshots using Google's nano banana image model. This is a great app to build if you're new to AI coding because it's just really fun.
```

**Why it fails:**
- Newsletter greeting
- Too casual ("just really fun")
- Marketing language
- No substance or depth
- Promotional tone

---

### Example 3: Providing Evidence

**GOOD (from Good #3):**
```
Estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used to make the estimate and the definition of what it means to know a word. A simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.
```

**Why it works:**
- Specific data range
- Acknowledges methodology variation
- Draws clear implication
- Concrete numbers (7-10 words/day)
- Logical progression

**BAD (Newsletter - Bad #1):**
```
Rigorous thinking is where flimsy ideas go to die. As a leader, having a team of rigorous thinkers is an organizational force multiplier that allows you to produce higher quality work, faster, even when resources are limited.
```

**Why it fails:**
- Corporate jargon ("force multiplier")
- No evidence or examples
- Marketing language
- Vague claims ("higher quality work")
- No substance behind assertions

---

## FREQUENTLY ASKED QUESTIONS

**Q: When should I use technical jargon?**
A: Use technical terms when they're the precise word for the concept (like "hallucination" for LLMs, "supervised learning"). Always define them on first use. Avoid corporate jargon that has simpler alternatives (use "use" not "leverage").

**Q: How formal should citations be?**
A: Use numbered brackets [1] or inline author-year (Dahl et al., 2024). Include References section at bottom with: Author/Organization, "Title", Year. No URLs in learner-facing content (track separately for verification).

**Q: Can I be conversational while staying professional?**
A: Yes! Use "you" to engage readers, occasional "I" for guidance, thoughtful questions, and genuine enthusiasm. Avoid: excessive casualness (gonna, super, emoji), marketing speak, or condescension.

**Q: How many examples should I include?**
A: At least one concrete, specific example for each major concept. Complex concepts may need 2-3 building in complexity. Quality over quantityâ€”one detailed example beats three vague ones.

**Q: Should I cite everything?**
A: Cite statistics, research findings, specific data, and non-common-knowledge facts. Don't cite: definitions of standard terms, common knowledge, or your own explanations. When in doubt, cite.

**Q: What's the difference between this and academic writing?**
A: Academic: Dense jargon, assumes expertise, formal citations, passive voice. This style: Accessible language, builds expertise, practical citations, active voice. Both are credible and evidence-based, but this prioritizes teaching.

---

## DISTINCTIVE FEATURES OF THIS STYLE

What makes this educational style unique:

**1. Evidence-Based Teaching**
- Research-backed claims
- Citations throughout
- Credibility without pomposity
- Academic rigor, accessible delivery

**2. Progressive Complexity**
- Starts simple, builds systematically
- Defines before using
- Examples before abstractions
- Respects learning process

**3. Strategic Engagement**
- Questions that prompt thinking
- Invitations to reflect
- Forward references create momentum
- Genuine enthusiasm for profound ideas

**4. Professional-Conversational Balance**
- Direct address ("you") without casualness
- Clear without being simplistic
- Authoritative without being distant
- Warm without being promotional

**5. Concrete Grounding**
- Specific examples with numbers
- Named datasets and scenarios
- Realistic, visualizable situations
- Evidence connects to application

**6. Teaching-First Orientation**
- Every element serves learning
- No marketing or self-promotion
- No unnecessary formatting tricks
- Substance over style

**This style works best for:**
- Educational courses for professionals
- Technical topics for non-experts
- Self-paced learning materials
- Research-based instruction
- Adult learners who value both understanding and credibility

**This style avoids:**
- Newsletter/marketing tone
- Pure academic paper density
- Tutorial blog casualness
- Corporate jargon
- Promotional elements

---

## ANALYSIS DETAILS

**Good Examples Analyzed:** 3 files
- good_1.txt: Supervised/Unsupervised ML (technical book chapter)
- good_2.txt: RAG and LLM Question Answering (academic textbook)
- good_3.txt: Vocabulary and Language Learning (academic textbook)

**Bad Examples Analyzed:** 3 files
- bad_1.txt: Rigorous Thinking Newsletter (too promotional, casual)
- bad_2.txt: Transformer Architecture Paper (too academic, dense)
- bad_3.txt: AI Tutorial Newsletter (too casual, promotional)

**Pattern Confidence:**

**HIGH Confidence (appears in all good examples):**
- Professional-conversational tone
- Evidence-based claims with citations
- Concrete examples with specific details
- Direct address to reader ("you")
- Teaching-first orientation
- Technical terms defined on first use

**MEDIUM Confidence (appears in most good examples):**
- Rhetorical questions for engagement
- Forward references to maintain interest
- Problem â†’ Solution structure
- Metaphors for key concepts
- Research citation patterns

**Distinctive vs Bad Examples:**
- Good: Research-backed, teaching-focused, accessible without casualness
- Bad #1 (Newsletter): Marketing tone, promotional, too casual
- Bad #2 (Academic): Dense jargon, assumes expertise, impenetrable
- Bad #3 (Tutorial): Too casual, promotional elements, lacks depth

---

**End of Default Style Guide**
